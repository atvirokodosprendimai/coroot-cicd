<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Coroot CI/CD</title>
<link rel="stylesheet" href="tufte.css"/>
<style>
  /* Force dark mode regardless of system preference */
  body { background-color: #151515; color: #ddd; }
  hr { border-top-color: #333; }
  pre > code { background: #1a1a1a; padding: 1rem; border-radius: 2px; }
  code { background: #1a1a1a; padding: 0.1em 0.3em; border-radius: 2px; }
  table { border-top: 2px solid #666; border-bottom: 2px solid #666; }
  th { border-bottom: 1px solid #444; }
</style>
</head>
<body>
<article>

<h1>Coroot CI/CD</h1>
<p class="subtitle">An automated update pipeline for the Coroot observability stack at table.beerpub.dev</p>

<section>
<div class="epigraph">
<blockquote>
<p>What gets measured gets managed.</p>
<footer>Attributed to Peter Drucker</footer>
</blockquote>
</div>

<p><span class="newthought">This system keeps</span> a Coroot observability
platform<label for="sn-coroot" class="margin-toggle sidenote-number"></label><input
type="checkbox" id="sn-coroot" class="margin-toggle"/><span class="sidenote"><a
href="https://coroot.com">Coroot</a> is an open-source observability platform that
combines metrics, logs, traces, and profiles into a single interface. It uses Prometheus
for metrics storage and ClickHouse for logs, traces, and profiles.</span>
automatically up to date. A GitHub Actions workflow runs every Monday at 04:00 UTC, compares
the running container image digests against the latest releases, and&mdash;when updates are
available&mdash;backs up all data, validates in a staging environment, deploys to production,
and rolls back automatically if anything goes wrong.</p>

<p>The pages in this documentation are:
the <a href="pipeline.html">pipeline history</a> page, which shows
recent workflow runs and per-job timing;
and the <a href="architecture.html">architecture diagram</a>, which maps the full system
from GitHub through the VPS to external services.
The <a href="https://github.com/atvirokodosprendimai/coroot-cicd">source repository</a>
contains the workflow definitions, deployment scripts, and this documentation.</p>
</section>

<section>
<h2>The Pipeline</h2>

<p><span class="newthought">The workflow consists</span> of six jobs that execute
sequentially.<label for="sn-sequential" class="margin-toggle sidenote-number"></label><input
type="checkbox" id="sn-sequential" class="margin-toggle"/><span class="sidenote">Each job
depends on the previous one via the <code>needs:</code> directive in the workflow YAML. If
staging validation fails, the deploy job is skipped entirely and production is never touched.
This is a deliberate design choice to prevent untested images from reaching production.</span>
The first checks for new images; the second backs up all eight Docker volumes; the third
deploys to a staging stack on the same VPS for validation; the fourth deploys to production;
the fifth rolls back if&mdash;and only if&mdash;the deploy job fails; and the sixth
cleans up staging artifacts and prunes dangling images.</p>

<pre><code>Schedule (Mon 04:00 UTC) or Manual Trigger
  │
  1. Check Updates     Compare running vs latest image digests
  │  → Exit early if all images current (unless force_deploy)
  │
  2. Backup Volumes    Snapshot all 8 Docker volumes (~330 MB)
  │  → Sync to Hetzner Storage Box (off-site)
  │  → Retain last 3 backups
  │
  3. Staging           Deploy to /opt/coroot-staging/, validate, tear down
  │  → Abort if staging fails (production untouched)
  │
  4. Deploy            docker compose pull && up -d
  │  → Health check via https://table.beerpub.dev
  │
  │──[on failure]──→ 5. Rollback    Restore volumes, restart, verify
  │
  6. Cleanup           Tear down staging, prune dangling images</code></pre>

<p>The rollback job deserves special attention.<label for="sn-rollback"
class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-rollback"
class="margin-toggle"/><span class="sidenote">During initial testing, the rollback job
triggered incorrectly when <em>staging</em> failed, even though production was never touched.
The fix was to replace the naive <code>if: failure()</code> condition&mdash;which evaluates
true when <em>any</em> upstream job fails&mdash;with the precise condition
<code>always() && needs.deploy.result == 'failure'</code>. This ensures rollback only fires
when the deploy job itself fails. See the
<a href="https://github.com/atvirokodosprendimai/coroot-cicd/blob/main/POSTMORTEM.md">postmortem</a>
for the full timeline.</span>
Its trigger condition is <code>always() && needs.deploy.result == 'failure'</code>, which
ensures it fires only when the deploy job fails&mdash;not when staging fails or when the
pipeline is cancelled. This distinction matters because an unnecessary rollback touches
production data (stops services, restores volumes) even when production is running fine.</p>
</section>

<section>
<h2>Infrastructure</h2>

<p><span class="newthought">The entire stack runs</span> on a single Hetzner
CAX11<label for="sn-vps" class="margin-toggle sidenote-number"></label><input
type="checkbox" id="sn-vps" class="margin-toggle"/><span class="sidenote">Hetzner CAX11:
ARM-based (Ampere Altra), 2 vCPU, 4 GB RAM, 40 GB disk, Ubuntu 24.04,
kernel 6.8.0-90-generic. Located in Falkenstein, Germany.
The VPS costs approximately &euro;3.79/month.</span>
(ARM, Ubuntu 24.04) at 91.99.74.36, reachable at
<a href="https://table.beerpub.dev">table.beerpub.dev</a>.</p>

<table>
<thead><tr><th>Service</th><th>Image</th><th>Role</th></tr></thead>
<tbody>
<tr><td>Coroot</td><td>ghcr.io/coroot/coroot</td><td>Observability platform</td></tr>
<tr><td>Prometheus</td><td>prom/prometheus:v2.53.5</td><td>Metrics storage</td></tr>
<tr><td>ClickHouse</td><td>clickhouse-server:24.3</td><td>Logs, traces, profiles</td></tr>
<tr><td>Node Agent</td><td>ghcr.io/coroot/coroot-node-agent</td><td>eBPF host metrics</td></tr>
<tr><td>Cluster Agent</td><td>ghcr.io/coroot/coroot-cluster-agent</td><td>Metrics scraper</td></tr>
<tr><td>Caddy</td><td>caddy:2</td><td>Reverse proxy, auto-TLS</td></tr>
</tbody>
</table>

<p>A critical detail: the production <code>docker-compose.yml</code> uses <code>expose:</code>
for all services except Caddy.<label for="sn-expose" class="margin-toggle sidenote-number"></label><input
type="checkbox" id="sn-expose" class="margin-toggle"/><span class="sidenote">Docker's
<code>expose:</code> directive opens a port only within the container network. Unlike
<code>ports:</code>, it does <em>not</em> bind the port to the host. This means
<code>curl http://localhost:8080</code> from the host returns &ldquo;connection refused&rdquo;
even when Coroot is running perfectly. This was the root cause of the first three pipeline
test failures&mdash;health check scripts assumed they could reach services via
localhost.</span>
This means services are not reachable from the host via <code>localhost:&lt;port&gt;</code>.
Only Caddy binds to ports 80 and 443 on the host. The external Caddy endpoint at
<code>https://table.beerpub.dev</code> is therefore the sole authoritative health check.</p>
</section>

<section>
<h2>Scripts</h2>

<p>Seven bash scripts perform the actual work on the VPS. Each supports a
<code>--dry-run</code> flag<label for="sn-dryrun" class="margin-toggle sidenote-number"></label><input
type="checkbox" id="sn-dryrun" class="margin-toggle"/><span class="sidenote">In dry-run mode,
the backup script estimates volume sizes without stopping services, the deploy script shows
current images and probes the endpoint without pulling or restarting anything, and the
rollback script lists backup contents without restoring. This enables safe validation of
script logic on production infrastructure.</span>
that reports what would happen without making changes. They are executed over SSH
from GitHub Actions:</p>

<pre><code>ssh root@91.99.74.36 'bash -s -- --dry-run' < scripts/deploy-production.sh</code></pre>

<table>
<thead><tr><th>Script</th><th>Purpose</th></tr></thead>
<tbody>
<tr><td><code>check-updates.sh</code></td><td>Compare running vs remote image digests</td></tr>
<tr><td><code>backup-volumes.sh</code></td><td>Snapshot all Docker volumes, prune old backups</td></tr>
<tr><td><code>deploy-staging.sh</code></td><td>Deploy staging stack, validate, tear down</td></tr>
<tr><td><code>deploy-production.sh</code></td><td>Pull latest images, deploy, health check</td></tr>
<tr><td><code>health-check.sh</code></td><td>Reusable HTTP probe (supports --staging, --external)</td></tr>
<tr><td><code>rollback.sh</code></td><td>Restore volumes from backup, restart, verify</td></tr>
<tr><td><code>sync-remote-backup.sh</code></td><td>Sync local backups to Hetzner Storage Box</td></tr>
</tbody>
</table>
</section>

<section>
<h2>Backups</h2>

<p><span class="newthought">Before every update,</span> all eight Docker volumes are compressed
and saved to <code>/opt/coroot/backups/&lt;timestamp&gt;/</code>.<label for="sn-backupsize"
class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-backupsize"
class="margin-toggle"/><span class="sidenote">Total compressed backup size is approximately
330 MB. ClickHouse data accounts for 327 MB of that. The backup/restore cycle takes about
three minutes: services are stopped, volumes are tarred via an Alpine container, and services
are restarted. The three-minute window is the primary contributor to observability gaps during
updates.</span>
The three most recent backups are retained locally; older ones are pruned automatically.
After the local backup completes, the pipeline syncs it to a Hetzner Storage
Box<label for="mn-storagebox" class="margin-toggle">&#8853;</label><input type="checkbox"
id="mn-storagebox" class="margin-toggle"/><span class="marginnote">The Storage Box uses
rsync over SSH on port 23, which is Hetzner's standard SSH port for storage boxes.
If the Storage Box is not configured, the sync step is skipped without failing the pipeline.
Configuration lives at <code>/etc/coroot-backup.conf</code> on the VPS.</span>
via rsync for off-site redundancy.</p>
</section>

<section>
<h2>Monitoring</h2>

<p>Three independent layers protect the system:</p>

<p>First, the pipeline itself probes <code>https://table.beerpub.dev</code> after every
deployment, with three retries at 15-second intervals. This is the authoritative health
check.<label for="sn-healthtiming" class="margin-toggle sidenote-number"></label><input
type="checkbox" id="sn-healthtiming" class="margin-toggle"/><span class="sidenote">Coroot's
Docker healthcheck takes over 120 seconds to converge on cold starts due to cache warming
and initial data population. Docker reports &ldquo;unhealthy&rdquo; even when the service
responds HTTP 200 through Caddy. The pipeline therefore treats the external HTTP response
as authoritative, regardless of Docker's healthcheck status.</span></p>

<p>Second, an independent GitHub Actions workflow runs every five minutes, probing the same
endpoint. If it fails three consecutive times, it creates a GitHub Issue with the label
<code>incident:downtime</code>. When the endpoint recovers, the issue is closed automatically.
This monitor runs on GitHub's infrastructure, not the VPS, so it detects downtime even if
the server is completely unreachable.</p>

<p>Third, GitGuardian's <code>ggshield</code> scans every commit for hardcoded secrets, both
as a pre-commit hook and as a CI workflow on push. This prevents credentials from entering
the repository.</p>
</section>

<section>
<h2>Quick Reference</h2>
<table>
<thead><tr><th>Resource</th><th></th></tr></thead>
<tbody>
<tr><td>Production</td><td><a href="https://table.beerpub.dev">https://table.beerpub.dev</a></td></tr>
<tr><td>Repository</td><td><a href="https://github.com/atvirokodosprendimai/coroot-cicd">atvirokodosprendimai/coroot-cicd</a></td></tr>
<tr><td>Workflow runs</td><td><a href="https://github.com/atvirokodosprendimai/coroot-cicd/actions">GitHub Actions</a></td></tr>
<tr><td>SSH</td><td><code>ssh -i ~/.ssh/coroot-table root@91.99.74.36</code></td></tr>
<tr><td>Stack path</td><td><code>/opt/coroot/</code></td></tr>
<tr><td>Backups</td><td><code>/opt/coroot/backups/</code></td></tr>
<tr><td>Schedule</td><td>Monday 04:00 UTC</td></tr>
<tr><td>Manual trigger</td><td>force_deploy, skip_staging, skip_backup, dry_run</td></tr>
</tbody>
</table>
</section>

</article>
</body>
</html>
